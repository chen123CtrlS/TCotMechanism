{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dicts(entities):\n",
    "    entity2ind = dict()\n",
    "    ind2entity = []\n",
    "    for i in range(len(entities)):\n",
    "        entity = entities[i]\n",
    "        if not (entity in ind2entity):\n",
    "            ind2entity.append(entity)\n",
    "            entity2ind[entity] = len(ind2entity) - 1\n",
    "    return ind2entity, entity2ind\n",
    "\n",
    "def choose(arr, ratio_or_count):\n",
    "    if type(ratio_or_count) == float:\n",
    "        num = round(ratio_or_count*len(arr))\n",
    "    elif type(ratio_or_count) == int:\n",
    "        num = ratio_or_count\n",
    "    else:\n",
    "         assert False\n",
    "    if num >= len(arr):\n",
    "        return arr\n",
    "    rand_inds = np.random.choice(len(arr), num, replace=False).tolist()\n",
    "    return [arr[i] for i in rand_inds]\n",
    "    \n",
    "def split(arr, ratio_or_count):\n",
    "    if type(ratio_or_count) == float:\n",
    "        num = round(ratio_or_count*len(arr))\n",
    "    elif type(ratio_or_count) == int:\n",
    "        num = ratio_or_count\n",
    "    else:\n",
    "         assert False\n",
    "    train, test = [], []\n",
    "    rand_inds = np.random.choice(len(arr), num, replace=False).tolist()\n",
    "    for i in tqdm(range(len(arr))):\n",
    "        if i in rand_inds:\n",
    "            train.append(arr[i])\n",
    "        else:\n",
    "            test.append(arr[i])\n",
    "    return [train, test]\n",
    "\n",
    "def form_items(c, t, b = None, b1 = None, noise = 0):\n",
    "    len_c = len(c)\n",
    "    input_text = \"\".join(c)\n",
    "    target_text = input_text + \"\".join([t, \"</a>\"])\n",
    "    item = None\n",
    "    if len_c == 4 and b1 != None:\n",
    "        input_text_cot = \"\".join(c)\n",
    "        target_text_cot = input_text_cot + \"\".join([b, b1, t, \"</a>\"])\n",
    "    \n",
    "        input_1 = \"\".join(c[:3]) # h1 r1 r2\n",
    "        target_1 = input_1 + \"\".join([b, b1, \"</a>\"])\n",
    "        input_2 = \"\".join([b]+c[2:4]) \n",
    "        target_2 = input_2 + \"\".join([b1, t, \"</a>\"])\n",
    "\n",
    "        item = {\n",
    "        \"input_text\": input_text_cot,\n",
    "        \"target_text\": target_text_cot,\n",
    "        'hop1':[input_1,target_1],\n",
    "        'hop2':[input_2,target_2]\n",
    "            }\n",
    "    elif len_c == 3 and b!= None: #inferred\n",
    "        # try1: h r1 <> r2 -> h r1 b r2 t\n",
    "        # input_text_cot = \"\".join(c[:2])+ \"<b>\" +c[-1]\n",
    "        # target_text_cot = input_text_cot.replace(\"<b>\",b) + \"\".join([t, \"</a>\"])\n",
    "        # try2: h r1 r2 -> h r1 r2 b t\n",
    "        input_text_cot = \"\".join(c)\n",
    "        target_text_cot = input_text_cot + \"\".join([b, t, \"</a>\"])\n",
    "        item = {\n",
    "        \"input_text\": input_text_cot,\n",
    "        \"target_text\": target_text_cot,\n",
    "        'train_noise': noise\n",
    "            }\n",
    "    else:\n",
    "        item = {\n",
    "        \"input_text\": input_text,\n",
    "        \"target_text\": target_text\n",
    "        }\n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 7396.25it/s]\n",
      "100%|██████████| 40000/40000 [00:00<00:00, 46232.17it/s]\n",
      "100%|██████████| 2000/2000 [00:03<00:00, 538.73it/s]\n",
      "100%|██████████| 2000/2000 [00:48<00:00, 41.45it/s]\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(num_entities, num_relations, out_degree=20, split_train_inferred=False):\n",
    " \n",
    "    entities = [\"<e_{}>\".format(i) for i in range(num_entities)]\n",
    "    ind2entity, entity2ind = build_dicts(entities)\n",
    "\n",
    "    relations = [\"<r_{}>\".format(i) for i in range(num_relations)]\n",
    "    ind2relation, relation2ind = build_dicts(relations)\n",
    "\n",
    "    atomic_dict = dict()   # maps a head entity to a list of (r, t) pairs\n",
    "    atomic_facts = []\n",
    "    atomics = []\n",
    "\n",
    "    for i in tqdm(range(num_entities)):\n",
    "        # for each subject entity, randomly select some outgoing relations to some random object entity\n",
    "        num_rows = out_degree\n",
    "        selected_rows = np.random.choice(num_relations, size=num_rows, replace=False).tolist()\n",
    "        for row_idx in selected_rows:\n",
    "            col_idx = np.random.randint(num_entities)  # pick some random tail entity for each selected (h,r)\n",
    "            h,r,t = ind2entity[i], ind2relation[row_idx], ind2entity[col_idx]\n",
    "            atomic_facts.append(form_items([h, r], t))\n",
    "            atomics.append((h,r,t))\n",
    "            if h not in atomic_dict:\n",
    "                atomic_dict[h] = []\n",
    "            atomic_dict[h].append((r, t))\n",
    "    if not split_train_inferred:\n",
    "        inferred_facts = []\n",
    "        for ent in tqdm(entities):\n",
    "            for (r1, b) in atomic_dict[ent]:\n",
    "                for (r2, t) in atomic_dict[b]:\n",
    "                    inferred_facts.append(form_items([ent, r1, r2], t, b))\n",
    "        return entities, relations, atomic_facts, inferred_facts\n",
    "    \n",
    "    # split ID/OOD\n",
    "    OOD_ratio = 0.05\n",
    "    OOD_facts, ID_facts = split(atomics, round(len(atomics)*OOD_ratio))\n",
    "    OOD_facts, ID_facts = set(OOD_facts), set(ID_facts)\n",
    "\n",
    "    id_atomic_facts = [form_items([h, r], t) for (h,r,t) in ID_facts]\n",
    "    ood_atomic_facts = [form_items([h, r], t) for (h,r,t) in OOD_facts]\n",
    "\n",
    "    # whether train with noise?\n",
    "    train_noise = True\n",
    "    lambda_noise = 0.1\n",
    "    noise_num = 0\n",
    "    train_inferred_facts, test_inferred_iid, test_inferred_ood = [], [], []\n",
    "    for ent in tqdm(entities):\n",
    "        for (r1, b) in atomic_dict[ent]:\n",
    "            for (r2, t) in atomic_dict[b]:\n",
    "                if (ent, r1, b) in OOD_facts or (b, r2, t) in OOD_facts:\n",
    "                    if (ent, r1, b) in OOD_facts and (b, r2, t) in OOD_facts:\n",
    "                        test_inferred_ood.append(form_items([ent, r1, r2], t, b))\n",
    "                    continue\n",
    "                if np.random.uniform() > 0.005:\n",
    "                    if train_noise:\n",
    "                        if np.random.rand()>lambda_noise:\n",
    "                            train_inferred_facts.append(form_items([ent, r1, r2], t, b))\n",
    "                        else:\n",
    "                            t_noise = random.choice(entities)\n",
    "                            b_noise = random.choice(entities)\n",
    "                            b = b_noise\n",
    "                            noise_num += 1\n",
    "                            train_inferred_facts.append(form_items([ent, r1, r2], t_noise, b, noise = 1))\n",
    "                    else:\n",
    "                        train_inferred_facts.append(form_items([ent, r1, r2], t, b))\n",
    "                else:\n",
    "                    test_inferred_iid.append(form_items([ent, r1, r2], t, b))\n",
    "    print(noise_num)\n",
    "\n",
    "    return entities, relations, id_atomic_facts, ood_atomic_facts, train_inferred_facts, test_inferred_iid, test_inferred_ood \n",
    "\n",
    "def build_dataset1(num_entities, num_relations, out_degree=20, split_train_inferred=False):\n",
    " \n",
    "    entities = [\"<e_{}>\".format(i) for i in range(num_entities)]\n",
    "    ind2entity, entity2ind = build_dicts(entities)\n",
    "\n",
    "    relations = [\"<r_{}>\".format(i) for i in range(num_relations)]\n",
    "    ind2relation, relation2ind = build_dicts(relations)\n",
    "\n",
    "    atomic_dict = dict()   # maps a head entity to a list of (r, t) pairs\n",
    "    atomic_facts = []\n",
    "    atomics = []\n",
    "\n",
    "    for i in tqdm(range(num_entities)):\n",
    "        # for each subject entity, randomly select some outgoing relations to some random object entity\n",
    "        num_rows = out_degree\n",
    "        selected_rows = np.random.choice(num_relations, size=num_rows, replace=False).tolist()\n",
    "        for row_idx in selected_rows:\n",
    "            col_idx = np.random.randint(num_entities)  # pick some random tail entity for each selected (h,r)\n",
    "            h,r,t = ind2entity[i], ind2relation[row_idx], ind2entity[col_idx]\n",
    "            atomic_facts.append(form_items([h, r], t))\n",
    "            atomics.append((h,r,t))\n",
    "            if h not in atomic_dict:\n",
    "                atomic_dict[h] = []\n",
    "            atomic_dict[h].append((r, t))\n",
    "    if not split_train_inferred:\n",
    "        inferred_facts = []\n",
    "        for ent in tqdm(entities):\n",
    "            for (r1, b) in atomic_dict[ent]:\n",
    "                for (r2, t) in atomic_dict[b]:\n",
    "                    inferred_facts.append(form_items([ent, r1, r2], t))\n",
    "        return entities, relations, atomic_facts, inferred_facts\n",
    "    \n",
    "    # split ID/OOD\n",
    "    OOD_ratio = 0.05\n",
    "    OOD_facts, ID_facts = split(atomics, round(len(atomics)*OOD_ratio))\n",
    "    OOD_facts, ID_facts = set(OOD_facts), set(ID_facts)\n",
    "\n",
    "    id_atomic_facts = [form_items([h, r], t) for (h,r,t) in ID_facts]\n",
    "    ood_atomic_facts = [form_items([h, r], t) for (h,r,t) in OOD_facts]\n",
    "\n",
    "    train_inferred_facts, test_inferred_iid, test_inferred_ood = [], [], []\n",
    "    for ent in tqdm(entities):\n",
    "        for (r1, b) in atomic_dict[ent]:\n",
    "            for (r2, t) in atomic_dict[b]:\n",
    "                if (ent, r1, b) in OOD_facts or (b, r2, t) in OOD_facts:\n",
    "                    if (ent, r1, b) in OOD_facts and (b, r2, t) in OOD_facts:\n",
    "                        test_inferred_ood.append(form_items([ent, r1, r2], t, b))\n",
    "                    continue\n",
    "                if np.random.uniform() > 0.005:\n",
    "                    train_inferred_facts.append(form_items([ent, r1, r2], t, b))\n",
    "                else:\n",
    "                    test_inferred_iid.append(form_items([ent, r1, r2], t, b))\n",
    "\n",
    "    # TODO:2-hop  --->  3-hop\n",
    "    train_3hop = False\n",
    "    iid_3hop=[]\n",
    "    ood_3hop=[]\n",
    "\n",
    "    for ent in tqdm(entities):\n",
    "        for (r1, b) in atomic_dict[ent]:\n",
    "            for (r2, c) in atomic_dict[b]:\n",
    "                for (r3, t) in atomic_dict[c]:\n",
    "                    if (ent, r1, b) in OOD_facts or (b, r2, c) in OOD_facts or (c, r3, t) in OOD_facts:\n",
    "                        if (ent, r1, b) in OOD_facts and (b, r2, c) in OOD_facts and (c, r3, t) in OOD_facts:\n",
    "                            ood_3hop.append(form_items([ent, r1, r2, r3], t, b, c))\n",
    "                        continue\n",
    "                    if np.random.uniform() > 0.005:\n",
    "                        if train_3hop:\n",
    "                            train_inferred_facts.append(form_items([ent, r1, r2, r3], t, b, c))\n",
    "                        else:\n",
    "                            continue\n",
    "                    else:\n",
    "                        iid_3hop.append(form_items([ent, r1, r2, r3], t, b, c))\n",
    "\n",
    "    return entities, relations, id_atomic_facts, ood_atomic_facts, train_inferred_facts, test_inferred_iid, test_inferred_ood, iid_3hop, ood_3hop \n",
    "    \n",
    "NUM_ENTITY_IN = 2000\n",
    "NUM_RELATION = 200\n",
    "\n",
    "train_entities, train_relations, id_atomic_facts, ood_atomic_facts, train_inferred_facts, test_inferred_iid, test_inferred_facts, iid_3hop, ood_3hop  = build_dataset1(NUM_ENTITY_IN, NUM_RELATION, split_train_inferred=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_text': '<e_0><r_110><r_73>', 'target_text': '<e_0><r_110><r_73><e_162><e_2></a>', 'train_noise': 0}\n",
      "{'input_text': '<e_0><r_17><r_153><r_93>', 'target_text': '<e_0><r_17><r_153><r_93><e_716><e_1016><e_1031></a>', 'hop1': ['<e_0><r_17><r_153>', '<e_0><r_17><r_153><e_716><e_1016></a>'], 'hop2': ['<e_716><r_153><r_93>', '<e_716><r_153><r_93><e_1016><e_1031></a>']}\n"
     ]
    }
   ],
   "source": [
    "print(test_inferred_iid[0])\n",
    "print(iid_3hop[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "718212"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "rand_num = np.random.rand()\n",
    "rand_num\n",
    "len(train_inferred_facts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 2207\n"
     ]
    }
   ],
   "source": [
    "vocab = []\n",
    "vocab = vocab + train_entities + train_relations\n",
    "# special tokens\n",
    "vocab = vocab + [\"<mask>\", \"<sep>\", \"<a>\", \"</a>\", \"<q>\", \"</q>\", \"<b>\"]\n",
    "assert len(vocab) == len(set(vocab))\n",
    "print(\"vocab size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size = 3000\n",
    "id_atomic_facts_ds = choose(id_atomic_facts, test_size)\n",
    "ood_atomic_facts_ds = choose(ood_atomic_facts, test_size)\n",
    "test_inferred_iid = choose(test_inferred_iid, test_size)\n",
    "test_inferred_facts_ds = choose(test_inferred_facts, test_size) #ood\n",
    "\n",
    "iid_3hop_ds = choose(iid_3hop, test_size) \n",
    "ood_3hop_ds = choose(ood_3hop, test_size) \n",
    "\n",
    "all_atomics = id_atomic_facts + ood_atomic_facts\n",
    "len(all_atomics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_text': '<e_3><r_109><r_112><r_145>',\n",
       " 'target_text': '<e_3><r_109><r_112><r_145><e_951><e_1482><e_1111></a>',\n",
       " 'hop1': ['<e_3><r_109><r_112>', '<e_3><r_109><r_112><e_951><e_1482></a>'],\n",
       " 'hop2': ['<e_951><r_112><r_145>',\n",
       "  '<e_951><r_112><r_145><e_1482><e_1111></a>']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ood_3hop_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsampling train_inferred\n",
    "# for phi in [18.0,12.6,9.0,7.2,5.4,3.6][:]:\n",
    "for phi in [12.6,3.6][:]:\n",
    "    dataset_name = \"composition1.{}.{}.{}\".format(NUM_ENTITY_IN, NUM_RELATION, phi)\n",
    "    os.makedirs(\"data/{}\".format(dataset_name), exist_ok=True)\n",
    "    train_inferred_facts_ds = choose(train_inferred_facts, round(phi * len(id_atomic_facts)))\n",
    "\n",
    "    probes = []\n",
    "    for item in id_atomic_facts_ds:\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1][\"type\"] = \"id_atomic\"\n",
    "    \n",
    "    for item in ood_atomic_facts_ds:\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1][\"type\"] = \"ood_atomic\"\n",
    "\n",
    "    for item in choose(train_inferred_facts_ds, test_size):\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1]['type'] = 'train_inferred'\n",
    "\n",
    "    for item in test_inferred_iid:\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1]['type'] = 'test_inferred_iid'\n",
    "\n",
    "    for item in test_inferred_facts_ds:\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1][\"type\"] = \"test_inferred_ood\"\n",
    "\n",
    "    with open(\"data/{}/train.json\".format(dataset_name), \"w\", encoding='utf-8') as f:  #all_atomics, inferred_facts_iid\n",
    "        json.dump(all_atomics + train_inferred_facts_ds, f)\n",
    "    with open(\"data/{}/valid.json\".format(dataset_name), \"w\", encoding='utf-8') as f:  #inferred_facts_ood\n",
    "        json.dump(test_inferred_facts_ds, f)\n",
    "    with open(\"data/{}/test.json\".format(dataset_name), \"w\", encoding='utf-8') as f: #iid,ood,atomic,inferred\n",
    "        json.dump(probes, f)\n",
    "    # add vocab\n",
    "    with open(\"data/{}/vocab.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(vocab, f)\n",
    "    \n",
    "    p_iid = []\n",
    "    for item in iid_3hop_ds:\n",
    "        p_iid.append(deepcopy(item))\n",
    "        p_iid[-1][\"type\"] = \"test_iid_3hop\"\n",
    "    p_ood = []\n",
    "    for item in ood_3hop_ds:\n",
    "        p_ood.append(deepcopy(item))\n",
    "        p_ood[-1][\"type\"] = \"test_ood_3hop\"\n",
    "    with open(\"data/{}/test_3hop_iid.json\".format(dataset_name), \"w\", encoding='utf-8') as f: #iid,ood,atomic,inferred\n",
    "        json.dump(p_iid, f)\n",
    "    with open(\"data/{}/test_3hop_ood.json\".format(dataset_name), \"w\", encoding='utf-8') as f: #iid,ood,atomic,inferred\n",
    "        json.dump(p_ood, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kkk = 0\n",
    "for item in train_inferred_facts_ds:\n",
    "    if item['train_noise'] == 1:\n",
    "        kkk+=1\n",
    "kkk/len(train_inferred_facts_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
